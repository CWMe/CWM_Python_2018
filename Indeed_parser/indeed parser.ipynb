{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the libraries that we will be using for these bits of code have been mentioned. \n",
    "\n",
    "The process is as follows : \n",
    "    1. Develop a function to get the information we need from a webpage \n",
    "    2. Develop a function to clean up the information we get from the webpage \n",
    "    3. Get a set of URLs we want to get information from into a list  \n",
    "    4. Run the above two functions for each URL in our list \n",
    "    5. Save the results to a csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indeed web scraper for job listings to apply for \n",
    "#import urllib\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parse the title , location , company , salary , synopsis , date of posting and link \n",
    "def parse(url):\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "    df = pd.DataFrame(columns=[\"Title\",\"Location\",\"Company\",\"Salary\", \"Synopsis\",\"date\",\"href\"])\n",
    "\n",
    "    for each in soup.find_all(class_= \"result\" ):\n",
    "            # get all the required data fields \n",
    "            try: \n",
    "                title = each.find(class_='jobtitle').text.replace('\\n', '')\n",
    "            except:\n",
    "                title = 'None'\n",
    "            try: \n",
    "                href = each.find(class_='jobtitle').find('a')['href']\n",
    "            except:\n",
    "                href = 'None'\n",
    "\n",
    "            try:\n",
    "                location = each.find('span', {'class':\"location\" }).text.replace('\\n', '')\n",
    "            except:\n",
    "                location = 'None'\n",
    "            try: \n",
    "                company = each.find(class_='company').text.replace('\\n', '')\n",
    "            except:\n",
    "                company = 'None'\n",
    "            try:\n",
    "                salary = each.find('span', {'class':'no-wrap'}).text.replace('\\n', '')\n",
    "            except:\n",
    "                salary = 'None'\n",
    "            try: \n",
    "                date = each.find('span', {'class':'date'}).text.replace('\\n', '')\n",
    "            except:\n",
    "                date = 'None'\n",
    "            synopsis = each.find('span', {'class':'summary'}).text.replace('\\n', '')\n",
    "            df = df.append({'href':href,'date':date, 'Title':title, 'Location':location, 'Company':company, 'Salary':salary, 'Synopsis':synopsis}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean up the dataframe , change data columns to what I need \n",
    "\n",
    "def transform_columns(link):\n",
    "    y = parse(link)\n",
    "    # replace the href to go to the job posting link \n",
    "    y['href'] = y['href'].replace('rc/clk','viewjob')\n",
    "    y['href'] = 'http://www.indeed.com/' + y['href']\n",
    "    \n",
    "    \n",
    "    # change the date to reflect the timestamp \n",
    "    y['date'] = y['date'].str.replace(' day ago','')\n",
    "    y['date'] = y['date'].str.replace(' days ago','')\n",
    "    y['date'] = y['date'].str.replace('Just posted','0')\n",
    "    y['date'] = y['date'].str.replace('Today','0')\n",
    "    y['date'] = y['date'].str.replace('None','30')\n",
    "    y['date'] = y['date'].str.replace('+','')\n",
    "    y.loc[y['date'].str.contains('hour', case=False), 'date'] = '0'\n",
    "    y.loc[y['date'].str.contains('minutes', case=False), 'date'] = '0'\n",
    "    y['date'] = y['date'].astype(str).astype(int)\n",
    "    y['date'] = datetime.datetime.now().date() -  pd.to_timedelta(y['date'], unit='d')\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_template = \"http://www.indeed.com/jobs?q={}+%2420%2C000&l={}&start={}\"\n",
    "links = []\n",
    "\n",
    "# define all the roles , locations , number of results for which I want to apply \n",
    "\n",
    "roles = ['data+scientist','data+engineer','machine+learning','data+analyst']\n",
    "\n",
    "locations = ['New+York', 'Chicago', 'San+Francisco', 'Austin', 'Seattle', \n",
    "    'Los+Angeles', 'Philadelphia', 'Atlanta', 'Dallas', 'Pittsburgh', \n",
    "    'Portland', 'Phoenix', 'Denver', 'Houston', 'Miami', 'Washington+DC', \n",
    "    'Charlottesville', 'Richmond', 'Baltimore', 'Harrisonburg', 'San+Antonio', 'San+Diego', 'San+Jose'\n",
    "    'Austin', 'Jacksonville', 'Indianapolis', 'Columbus', 'Fort+Worth', 'Charlotte', 'Detroit', 'El+Paso', \n",
    "    'Memphis', 'Boston', 'Nashville', 'Louisville', 'Milwaukee', 'Las+Vegas', 'Albuquerque', 'Tucson', \n",
    "    'Fresno', 'Sacramento', 'Long+Beach', 'Mesa', 'Virginia+Beach', 'Norfolk', 'Atlanta', 'Colorado+Springs',\n",
    "    'Raleigh', 'Omaha', 'Oakland', 'Tulsa', 'Minneapolis', 'Cleveland', 'Wichita', 'Arlington', 'New+Orleans', \n",
    "    'Bakersfield', 'Tampa', 'Honolulu', 'Anaheim', 'Aurora', 'Santa+Ana', 'Riverside', 'Corpus+Christi', 'Pittsburgh', \n",
    "    'Lexington', 'Anchorage', 'Cincinnati', 'Baton+Rouge', 'Chesapeake', 'Alexandria', 'Fairfax', 'Herndon',\n",
    "    'Reston', 'Roanoke','Orlando']\n",
    "start = ['0','50','100','150','200']\n",
    "\n",
    "for i in range(0,len(roles)):\n",
    "    for j in range(0,len(locations)):\n",
    "        for k in range(0,len(start)):\n",
    "            links.append(url_template.format(roles[i],locations[j],start[k]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1054/1054 [48:12<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished parsing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(columns=[\"Title\",\"Location\",\"Company\",\"Salary\", \"Synopsis\",\"date\",\"href\"])\n",
    "\n",
    "for i in tqdm(range(0,len(links))):\n",
    "    df = df.append(transform_columns(links[i]))\n",
    "    # drop duplicate entries based on synopsis and company name \n",
    "    # the logic here is that these two are enough to identify what the unique entries are . \n",
    "    \n",
    "    df = df.drop_duplicates([\"Company\",\"Synopsis\"])\n",
    "    time.sleep(2)\n",
    "print('finished parsing')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('test1.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "#The jane austen word counter logic  \n",
    "# download the book . Project Gutenberg has a txt version of the entire book . Get it . \n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book = requests.get('https://www.gutenberg.org/files/1342/1342.txt').content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = str(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sentence(sent):\n",
    "    return({word: True for word in nltk.word_tokenize(sent)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nltk.download' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# format_sentence(\"this is a sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('elizabeth', 587), ('darcy', 349), ('bennet', 279), ('jane', 252), ('bingley', 243), ('never', 219)]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "from string import whitespace, punctuation\n",
    "r = requests.get(\"https://www.gutenberg.org/files/1342/1342-h/1342-h.htm\")\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "text = (word.lower().strip(whitespace + punctuation)\n",
    "        for element in soup.findAll('p')\n",
    "        for text in element.findAll(text=True)\n",
    "        for word in text.split())\n",
    "\n",
    "words_to_exclude = list(set(stopwords.words('english')))\n",
    "words_to_exclude.append('mr')\n",
    "words_to_exclude.append('could')\n",
    "words_to_exclude.append('would')\n",
    "words_to_exclude.append('said')\n",
    "words_to_exclude.append('mrs')\n",
    "words_to_exclude.append('much')\n",
    "words_to_exclude.append('must')\n",
    "words_to_exclude.append('“i')\n",
    "words_to_exclude.append('miss')\n",
    "words_to_exclude.append('one')\n",
    "words_to_exclude.append('know')\n",
    "\n",
    "\n",
    "#{'knew','a', 'or', 'the', 'of', 'it', 'is', 'it', 'and', 'in', 'she', 'he', 'to', 'i',\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\",'ma\\'am'}\n",
    "\n",
    "important = Counter(word for word in text if word not in words_to_exclude)\n",
    "\n",
    "print(important.most_common(6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "# ( if you don't have the corpus already downloaded , run this once')\n",
    "# nltk.download('stopwords')\n",
    "list1 = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'herself'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
